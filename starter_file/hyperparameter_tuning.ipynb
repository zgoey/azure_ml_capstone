{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning using HyperDrive\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1598531914256
    }
   },
   "outputs": [],
   "source": [
    "from azureml.core import Workspace, Experiment\n",
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.widgets import RunDetails\n",
    "from azureml.train.hyperdrive.run import PrimaryMetricGoal\n",
    "from azureml.train.hyperdrive.policy import BanditPolicy\n",
    "from azureml.train.hyperdrive.sampling import RandomParameterSampling\n",
    "from azureml.train.hyperdrive.runconfig import HyperDriveConfig\n",
    "from azureml.train.estimator import Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creation of compute cluster to carry our the automated ML\n",
    "compute_name = \"udacity-cluster\"\n",
    "try:\n",
    "    compute = ComputeTarget(workspace=ws, name=compute_name)\n",
    "    print('Compute cluster {} already exists!'.format(compute_name))\n",
    "except ComputeTargetException:\n",
    "    config = AmlCompute.provisioning_configuration(vm_size='STANDARD_D2_V2', max_nodes=4)\n",
    "    compute = ComputeTarget.create(ws, compute_name, config)\n",
    "    \n",
    "compute.wait_for_completion()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Get data. In the cell below, write code to access the data you will be using in this project. Remember that the dataset needs to be external.\n",
    "url = \"https://github.com/zgoey/azure_ml_capstone/blob/master/color_shades.csv\"\n",
    "dataset = TabularDatasetFactory.from_delimited_files(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1598531917374
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "ws = Workspace.from_config()\n",
    "experiment_name = 'udacity-capstone'\n",
    "\n",
    "experiment=Experiment(ws, experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.register(workspace = ws, name = 'color_shades',\n",
    "                                 description = 'RGB values labeled with color shade names',\n",
    "                                 create_new_version = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "gather": {
     "logged": 1598531923519
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Hyperdrive Configuration\n",
    "\n",
    "For this hyperdrive experiment we will use a K-nearest neighbor classifier , which is simple and at the same time flexible enough to adapt itself to irregular decision boundaries. For a K-nearest neighvor classifier it is always a bit of challenge to find the optimal number of neighbors, so that is one of the parameters, over which we will sample. We will also investigate the effect of weighting samples by their distances as opposed to giving all neighbors the same weight by including the 'weights' parameter in our search . Finally, the distance measure is something that is interesting to vary. Here we will takean educated guess. Instead of varyting the metric, we will compare a direct kNN application to one preceded by an embedding into the perceptually uniform LaB space (see) and  a metric learning algporithm that Neighborhood Components Analysis (see https://scikit-learn.org/stable/modules/neighbors.html, section 1.6.7), which has been devised an an improvement kNN with the standrad Eucildean distance metric.\n",
    "\n",
    "We use Bayesian parameter sampling, because our hyperparamter sample space is relatively small and we have enough budget to explore. We do no set an early termination policy, because this is not supported when using Bayesian sampling. The maximal number of runs is set to 150, which is according to the recommendation to the recoomendation to set the maximum number of runs greater than or equal to 20 times the number of hyperparameters being tuned (see https://docs.microsoft.com/en-us/azure/machine-learning/how-to-tune-hyperparameters#define-search-space). The maximum number of concurrent runs is set to 1, to let each run beenfit fullt from previously completed runs, which will enhance the sampling convergence.\n",
    "\n",
    "We set the primary metric to accuracy, which is the most common measure used for classification tasks. Further details of the traning procesure can be found in the file train.py, which defined the estimator that we use in our experiment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1598544893076
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Create an early termination policy. This is not required if you are using Bayesian sampling.\n",
    "\n",
    "#TODO: Create the different params that you will be using during training\n",
    "param_sampling =   BayesianParameterSampling(\n",
    "    {\n",
    "        '--n_neighbors': choice(range(1,101)), \n",
    "        '--weights': choice('uniform', 'distance'),\n",
    "        '--embedding': choice('none', 'lab', 'nca')\n",
    "    }\n",
    "\n",
    "#TODO: Create your estimator and hyperdrive config\n",
    "estimator = Estimator(source_directory=\".\",\n",
    "                entry_script=\"train.py\",\n",
    "                compute_target=compute\n",
    "               )\n",
    "\n",
    "hyperdrive_run_config = HyperDriveConfig(\n",
    "    estimator=estimator,\n",
    "    hyperparameter_sampling=param_sampling,\n",
    "    primary_metric_name='Accuracy',\n",
    "    primary_metric_goal=PrimaryMetricGoal.MAXIMIZE\n",
    "    max_total_runs = 150,\n",
    "    max_concurrent_runs = 1,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1598544897941
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "#TODO: Submit your experiment\n",
    "hyperdrive_run = experiment.submit(hyperdrive_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "gather": {
     "logged": 1598544898497
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Run Details\n",
    "\n",
    "OPTIONAL: Write about the different models trained and their performance. Why do you think some models did better than others?\n",
    "\n",
    "TODO: In the cell below, use the `RunDetails` widget to show the different experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1598546648408
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "RunDetails(hyperdrive_run).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperdrive_run.wait_for_completion()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1598546650307
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "best_run = hyperdrive_run.get_best_run_by_primary_metric()\n",
    "print(best_run.get_metrics())\n",
    "print(best_run.get_file_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1598546657829
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "#TODO: Save the best model\n",
    "best_run.download_file(name='outputs/model.pkl', output_file_path='./models/hyperdrive_color_shades.pkl')\n",
    "best_run.register_model(model_name='hyperdrive_color_shades', model_path='outputs/model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Model Deployment\n",
    "\n",
    "Remember you have to deploy only one of the two models you trained.. Perform the steps in the rest of this notebook only if you wish to deploy this model.\n",
    "\n",
    "TODO: In the cell below, register the model, create an inference config and deploy the model as a web service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: In the cell below, send a request to the web service you deployed to test it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: In the cell below, print the logs of the web service and delete the service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3-azureml"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
